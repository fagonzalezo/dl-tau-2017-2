---
id: resources
name: Resources
heading: Course resources
subheading: 
image: 
---
### Computing Resources
Thanks to the generous sponsorship of Microsoft Research, the course will have access to the cloud platform Azure to support experimentation for the assignments. More information about this will become available soon.

### Courses

* [NLPDL-Stanford] [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/), Stanford University, Spring 2017
* [DLNLP-Oxford] [Deep Learning for Natural Language Processing: 2016-2017](https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/), University of Oxford, 2016-2017
* [DLNLP-UCSB] [CS292F: Deep Learning for NLP](http://william.cs.ucsb.edu/courses/index.php/Spring_2017_CS292F_Deep_Learning_for_NLP), University of California at Santa Barbara, Spring 2017

### Tools

* [TensorFlow](https://www.tensorflow.org/): An open-source software library for Machine Intelligence
  * [Installation](https://www.tensorflow.org/install/)
  * [Getting started](https://www.tensorflow.org/get_started/)
  * [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)
  * [Programmer's Guide](https://www.tensorflow.org/programmers_guide/?nav=true)
  * [Vector Representations of Words](https://www.tensorflow.org/tutorials/word2vec)
  * [Recurrent NNs](https://www.tensorflow.org/tutorials/recurrent)
  * [Sequence-to-Sequence Models](https://www.tensorflow.org/tutorials/seq2seq)
* [Keras](https://keras.io/): The Python Deep Learning library
  * [Installation](https://keras.io/#installation)
  * [Getting started](https://keras.io/getting-started/sequential-model-guide/)
  * [Text preprocessing](https://keras.io/preprocessing/text/)
  * [Pre-trained word embeddings in Keras](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)
  * [Classifying Yelp Reviews](http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/)
  * [Keras language modeling](https://github.com/codekansas/keras-language-modeling)
  * [Using Keras as part of a TensorFlow workflow](https://github.com/fchollet/keras-resources)
  * [More Keras resources](https://github.com/fchollet/keras-resources)

### References

*  **Text Book:** [Goodfellow2016]  Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep learning](http://www.deeplearningbook.org/). MIT Press.
* **Text Book:** [JM2017] Jurafsky,  D. and  Martin, J. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/), 3rd edition draft Chapters.
* [AMLS17] Aguilar, G.,  Maharjan, S.,  Lopez Monroy, A.P.,  and  Solorio, T. A Multi-task Approach for Named Entity Recognition in Social Media Data. Proceedings of the 3rd Workshop on Noisy User-generated Text. Copenhagen, Denmark, pp 148-153. 2017. ([paper](http://noisy-text.github.io/2017/pdf/WNUT19.pdf))
* [DNEL17] Derczynski, L., Nichols, E., van Erp, M. & Limsopatham, N. Results of the WNUT2017 Shared Task on Novel and
Emerging Entity Recognition. In Proceedings of the 3rd Workshop on Noisy, User-generated Text (W-NUT) at EMNLP ([paper](http://www.derczynski.com/sheffield/papers/emerging-wnut.pdf))
* [MAMGS17] Suraj Maharjan, John Arevalo, Manuel Montes and Fabio A. Gonzalez and Thamar Solorio. A Multi-task Approach to Predict Likability of Books. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pp 1217-1227. Valencia, Spain, 2017. ([paper](http://www.aclweb.org/anthology/E17-1114))
* [MSCCD13] Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J. Distributed Representations of Words and Phrases and their Compositionality. arXiv:1310.4546 ([paper](https://arxiv.org/abs/1310.4546))
* [PSM14] Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543). ([paper](http://www.aclweb.org/anthology/D14-1162))
* [R2014] X. Rong, (2014). Word2Vec Parameter learning explained. arXiv:1411.2738 ([paper](https://arxiv.org/abs/1411.2738))
* [YYDHSH16] Yang, Z., Yang, D., Dyer, C., He, X., Smola, A. and Hovy, E. Hierarchical Attention Networks for Document Classification. Proceedings of NAACL-HLT 2016, pages 1480-1489. San Diego, California, June 2016. ([paper](http://www.aclweb.org/anthology/N16-1174))
